# üéØ Complexity-Guided Ensemble - Documenta√ß√£o T√©cnica

## üìã √çndice

1. [Vis√£o Geral](#vis√£o-geral)
2. [Arquitetura](#arquitetura)
3. [Algoritmo Detalhado](#algoritmo-detalhado)
4. [Componentes Principais](#componentes-principais)
5. [Guia de Uso](#guia-de-uso)
6. [Exemplos Avan√ßados](#exemplos-avan√ßados)
7. [Performance e Benchmarks](#performance-e-benchmarks)
8. [Refer√™ncias Te√≥ricas](#refer√™ncias-te√≥ricas)

---

## üéì Vis√£o Geral

### O Que √â?

O **Complexity-Guided Ensemble** √© um m√©todo avan√ßado de ensemble learning que combina:

1. **ComplexityGuidedSampler** para gera√ß√£o de bags
2. **Varia√ß√£o sistem√°tica de complexidade** atrav√©s do par√¢metro Œº
3. **Diversidade** entre membros do ensemble
4. **Suporte completo a classifica√ß√£o multiclasse**

### Por Que Usar?

‚úÖ **Superior a m√©todos tradicionais** em dados desbalanceados  
‚úÖ **Diversidade aumentada** sem configura√ß√£o manual  
‚úÖ **Suporte nativo a multiclasse** com qualquer n√∫mero de classes  
‚úÖ **Escal√°vel** com paraleliza√ß√£o nativa  
‚úÖ **Interpret√°vel** atrav√©s dos valores Œº  

### Diferen√ßas dos M√©todos Tradicionais

| Aspecto | Bagging Tradicional | Random Forest | **CG-Ensemble** |
|---------|---------------------|---------------|-----------------|
| Diversidade | Aleat√≥ria | Aleat√≥ria + Feature sampling | **Guiada por complexidade** |
| Balanceamento | N√£o | N√£o | **Autom√°tico (CG-Sampler)** |
| Sele√ß√£o de inst√¢ncias | Aleat√≥ria | Aleat√≥ria | **Guiada por complexidade** |
| Multiclasse | Sim | Sim | **Sim** |
| Interpretabilidade dos bags | Baixa | Baixa | **Alta (Œº values)** |

---

## ‚ö° QUICK START GUIDE

### Instalar Depend√™ncias
```bash
pip install numpy pandas scikit-learn joblib pyhard
```

### Testar algoritmo com exemplos de demonstra√ß√£o
```bash
# Testar sampler
python demo_resampler.py

# Testar ensemble  
python demo_ensemble.py
```

### Primeiro C√≥digo

```python
# Exemplo m√≠nimo do sampler
from complexity_sampler_refactored import ComplexityGuidedSampler
from sklearn.datasets import make_classification
import numpy as np

X, y = make_classification(n_samples=1000, n_classes=2, weights=[0.9, 0.1])
sampler = ComplexityGuidedSampler(random_state=42)
X_bal, y_bal = sampler.fit_resample(X, y, mu=0.5, sigma=0.2, k_neighbors=5)

print(f"Antes: {np.bincount(y)}")  # [900, 100]
print(f"Depois: {np.bincount(y_bal)}")  # [500, 500]
```

```python
# Exemplo m√≠nimo do ensemble
from complexity_guided_ensemble import ComplexityGuidedEnsemble

ensemble = ComplexityGuidedEnsemble(n_estimators=5, random_state=42)
ensemble.fit(X, y)
predictions = ensemble.predict(X)

print(f"Treinado com sucesso! Precis√£o: {(predictions == y).mean():.2f}")
```

---



## üìê Algoritmo Detalhado

### Pseudoc√≥digo Completo

```python
Algorithm: ComplexityGuidedEnsemble

Input:
    D = {(x_i, y_i)}_{i=1}^N  # Training data
    n_estimators              # Number of base classifiers
    œÉ                         # Gaussian std (fixed)
    ComplexityMeasure         # Type of complexity metric

Output:
    E = {h_1, h_2, ..., h_n}  # Ensemble of classifiers

# Step 1: Generate Œº values systematically
Œº_values ‚Üê linspace(0, 1, n_estimators)

# Step 2: Initialize components
sampler ‚Üê ComplexityGuidedSampler(ComplexityMeasure)
estimators ‚Üê []

# Step 3: For each Œº value, create a base classifier
for i in 1 to n_estimators:
    Œº_i ‚Üê Œº_values[i]
    
    # 3.1: Calculate complexities
    complexities ‚Üê sampler.calculate_complexities(D)
    
    # 3.2: Generate weights with Gaussian
    weights ‚Üê exp(-((complexities - Œº_i)¬≤ / (2 * œÉ¬≤)))
    
    # 3.3: Separate classes
    D_classes ‚Üê separate_by_class(D)
    
    # 3.4: Resample each class
    D_resampled ‚Üê []
    for each class c in D_classes:
        # Undersample or oversample based on target size
        D_c_resampled ‚Üê weighted_resample(D_classes[c], weights, n_target)
        D_resampled ‚Üê D_resampled ‚à™ D_c_resampled
    
    # 3.5: Train base classifier
    h_i ‚Üê train_classifier(D_resampled)
    
    # 3.6: Store
    estimators.append(h_i)

return E = estimators
```

### Fun√ß√£o de Predi√ß√£o

```python
Algorithm: Predict

Input:
    x_test       # Test instance
    E            # Trained ensemble
    voting       # 'soft' or 'hard'
    classes      # All possible classes

Output:
    y_pred       # Predicted label

if voting == 'soft':
    # Average probabilities across all classes
    all_probas ‚Üê []
    
    for h_i in E:
        # Get probabilities from estimator
        proba_i ‚Üê h_i.predict_proba(x_test)
        
        # Align with all classes (some estimators may not have seen all classes)
        aligned_proba ‚Üê align_probabilities(proba_i, h_i.classes_, classes)
        all_probas.append(aligned_proba)
    
    # Average and normalize
    avg_proba ‚Üê mean(all_probas, axis=0)
    avg_proba ‚Üê avg_proba / sum(avg_proba)
    
    y_pred ‚Üê classes[argmax(avg_proba)]
else:
    # Majority vote
    votes ‚Üê [h_i.predict(x_test) for h_i in E]
    y_pred ‚Üê mode(votes)

return y_pred
```

---

## üß© Componentes Principais

### 1. ComplexityGuidedSampler

**Responsabilidade:** Gerar subconjuntos balanceados guiados por complexidade

**M√©tricas de Complexidade Dispon√≠veis:**

#### a) Overlap Complexity
```python
# Mede sobreposi√ß√£o de features entre classes
# Inst√¢ncias na regi√£o de overlap t√™m alta complexidade

complexity = distance_to_overlap_center(instance)
```

**Quando usar:** Dados com separa√ß√£o clara mas com regi√µes de overlap

**Multiclasse:** Calcula overlap entre todos os pares de classes

#### b) Error Rate Complexity
```python
# Mede dificuldade de classifica√ß√£o via cross-validation
# Inst√¢ncias mal classificadas t√™m alta complexidade

complexity = 1 - P(y=true_class|x)
```

**Quando usar:** Quer priorizar inst√¢ncias dif√≠ceis de classificar

**Multiclasse:** Usa probabilidade da classe verdadeira (funciona para qualquer n√∫mero de classes)

#### c) Neighborhood Complexity
```python
# Mede homogeneidade da vizinhan√ßa
# Inst√¢ncias em regi√µes mistas t√™m alta complexidade

complexity = count_different_class_neighbors(instance) / k_neighbors
```

**Quando usar:** Dados com clusters bem definidos

**Multiclasse:** Conta vizinhos de classes diferentes (n√£o apenas da classe majorit√°ria)


#### d) hardness_function
```python
# Permite a configura√ß√£o de uma fun√ß√£o de complexidade personalizada
# Neste caso usamos as fun√ß√µes disponibilizadas pela biblioteca 'pyhard'

complexity = hardness_function(instance)
```
**Quando usar:** Quando necessitar de uma meidada de complexidade diferente das fornecidas


### 2. Gaussian Weighting Function

```python
def gaussian_weight(complexity, mu, sigma):
    """
    Calcula peso Gaussiano para cada inst√¢ncia
    
    Parameters:
    -----------
    complexity : float [0, 1]
        Complexidade normalizada da inst√¢ncia
    mu : float [0, 1]
        Centro da Gaussiana (n√≠vel de complexidade alvo)
    sigma : float
        Desvio padr√£o (controla "spread")
    
    Returns:
    --------
    weight : float
        Peso da inst√¢ncia para amostragem
    """
    return np.exp(-((complexity - mu)**2) / (2 * sigma**2))
```

**Interpreta√ß√£o de Œº:**
- **Œº = 0.0:** Foca em inst√¢ncias simples (f√°ceis de classificar)
- **Œº = 0.5:** Foca em inst√¢ncias de dificuldade m√©dia
- **Œº = 1.0:** Foca em inst√¢ncias complexas (dif√≠ceis de classificar)

**Interpreta√ß√£o de œÉ:**
- **œÉ pequeno (0.1):** Sele√ß√£o muito focada, pouca varia√ß√£o
- **œÉ m√©dio (0.2-0.3):** Balanceado
- **œÉ grande (0.5):** Sele√ß√£o mais abrangente, menos espec√≠fica

### 3. Voting Strategies

#### a) Soft Voting (Recomendado)
```python
# M√©dia das probabilidades preditas
# Mais robusto e informativo

proba_final = mean([clf.predict_proba(x) for clf in estimators])
y_pred = argmax(proba_final)
```

**Vantagens:**
- Usa toda a informa√ß√£o dispon√≠vel (probabilidades)
- Mais est√°vel com classes desbalanceadas
- Melhor para multiclasse

#### b) Hard Voting
```python
# Voto majorit√°rio das predi√ß√µes
# Mais simples e r√°pido

votes = [clf.predict(x) for clf in estimators]
y_pred = mode(votes)
```

**Vantagens:**
- Mais r√°pido (n√£o precisa calcular probabilidades)
- Funciona com qualquer classificador
- Mais interpret√°vel

---

## üìö Guia de Uso

### Instala√ß√£o

```bash
pip install numpy pandas scikit-learn joblib
```

### Uso B√°sico 
```python
# Criar dados multiclasse desbalanceados
X, y = make_classification(
    n_samples=1000,
    n_classes=4,
    n_clusters_per_class=1,
    weights=[0.5, 0.3, 0.15, 0.05],
    n_informative=10,
    random_state=42
)

# Criar ensemble 
ensemble = ComplexityGuidedEnsemble(
    n_estimators=15,
    complexity_type='neighborhood',
    voting='soft',
    random_state=42
)

# Treinar
ensemble.fit(X, y)

# Predizer
y_pred = ensemble.predict(X)
y_proba = ensemble.predict_proba(X)

print(f"Number of classes: {ensemble.n_classes_}")  # 4
print(f"Probabilities shape: {y_proba.shape}")  # (n_samples, 4)
```

### Configura√ß√£o Avan√ßada

```python
from complexity_guided_ensemble_simplified import EnsembleConfig
from sklearn.tree import DecisionTreeClassifier

# Criar configura√ß√£o customizada
config = EnsembleConfig(
    n_estimators=20,                    # Mais estimadores
    base_estimator=DecisionTreeClassifier,
    complexity_type='error_rate',       # Usar taxa de erro
    sigma=0.3,                          # Spread maior
    k_neighbors=7,                      # Mais vizinhos para synthetic
    voting='soft',                      # Vota√ß√£o por probabilidade
    cv_folds=5,                         # CV folds para complexity
    n_jobs=-1,                          # Usar todos os cores
    verbose=1                           # Mostrar progresso
)

# Usar configura√ß√£o
ensemble = ComplexityGuidedEnsemble(config=config)
ensemble.fit(X, y)
```

---

## üéØ Exemplos Avan√ßados

### Exemplo: Compara√ß√£o com Baselines

```python
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier
from sklearn.model_selection import cross_val_score
import numpy as np

# Criar dados desbalanceados
X, y = make_classification(
    n_samples=1000,
    n_classes=3,
    n_clusters_per_class=1,
    weights=[0.7, 0.2, 0.1],
    random_state=42
)

methods = {
    'Random Forest': RandomForestClassifier(
        n_estimators=10, 
        random_state=42
    ),
    'Bagging': BaggingClassifier(
        n_estimators=10, 
        random_state=42
    ),
    'CG-Ensemble': ComplexityGuidedEnsemble(
        n_estimators=10,
        random_state=42
    )
}

print("Cross-validation results (F1-weighted):")
print("-" * 50)
for name, clf in methods.items():
    scores = cross_val_score(clf, X, y, cv=5, scoring='f1_weighted')
    print(f"{name:20s}: {scores.mean():.4f} ¬± {scores.std():.4f}")
```
---